{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ MAS Evaluation Framework Demo\n",
    "\n",
    "This notebook implements a complete **Multi-Agent System (MAS)** evaluation pipeline using **GEMMAS** (Graph-based Metrics) and **MAST** (Failure Taxonomy).\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Trace Capture**: Instrumentation using OpenTelemetry & Google ADK.\n",
    "2. **Execution**: Running a multi-agent scenario (Researcher & Writer).\n",
    "3. **Graph Construction**: Building a Causal Reasoning Graph (CRG) from traces.\n",
    "4. **GEMMAS Evaluation**: Computing Information Diversity Score (IDS) and Unnecessary Path Ratio (UPR).\n",
    "5. **MAST Analysis**: Self-correcting failure diagnosis (Fine-tune -> Classify).\n",
    "6. **Advisory**: Generating architectural recommendations using Gemini 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mas_eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1550706301.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Import MAS Eval Framework\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmas_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTraceData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmas_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madk_adapter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mADKAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mADKTracingCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmas_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrg_builder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCRGModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mas_eval'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure google-generativeai is installed\n",
    "# !pip install google-generativeai google-adk networkx sentence-transformers scikit-learn\n",
    "\n",
    "# Import MAS Eval Framework\n",
    "from mas_eval.core.types import Span, TraceData\n",
    "from mas_eval.adapters.adk_adapter import ADKAdapter, ADKTracingCallback\n",
    "from mas_eval.graph.crg_builder import CRGModule\n",
    "from mas_eval.metrics.gemmas import GEMMAS_Evaluator\n",
    "from mas_eval.mast.fine_tuning import MASTFineTuner\n",
    "from mas_eval.mast.classifier import MASTClassifier, ClassifierMode\n",
    "from mas_eval.suggestions.advisor import MASAdvisor\n",
    "from mas_eval.graph.visualizer import GraphVisualizer\n",
    "\n",
    "# Set API Key (User should replace this or set env var)\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è Please set GOOGLE_API_KEY environment variable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Trace Capture & Agent Setup\n",
    "We use `ADKTracingCallback` to capture every thought, action, and output from our agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import Agent\n",
    "from google.adk.runners import InMemoryRunner\n",
    "\n",
    "# Define Agent Instructions\n",
    "RESEARCHER_PROMPT = \"\"\"\n",
    "You are a Research Agent.\n",
    "Role: Gather technical information on the given topic.\n",
    "Output: a concise list of key facts.\n",
    "\"\"\"\n",
    "\n",
    "WRITER_PROMPT = \"\"\"\n",
    "You are a Writer Agent.\n",
    "Role: Synthesize the research into a short, engaging paragraph.\n",
    "Input: Research facts.\n",
    "Output: Final summary.\n",
    "\"\"\"\n",
    "\n",
    "# Create Agents\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "researcher = Agent(name=\"Researcher\", model=model_name, system_prompt=RESEARCHER_PROMPT)\n",
    "writer = Agent(name=\"Writer\", model=model_name, system_prompt=WRITER_PROMPT)\n",
    "\n",
    "# Initialize Tracing Callback\n",
    "tracer = ADKTracingCallback(service_name=\"mas-demo\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Execution Loop\n",
    "We run the agents in a simplified sequence to generate interaction traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_scenario(topic: str):\n",
    "    print(f\"‚ñ∂Ô∏è Starting Scenario: {topic}\")\n",
    "    tracer.clear()\n",
    "    \n",
    "    # 1. Researcher Step\n",
    "    print(\"\\n--- Researcher Working ---\")\n",
    "    # Manually simulating ADK events for demo structure clarity \n",
    "    # In real usage, you'd attach tracer to the runner\n",
    "    \n",
    "    # Simulate Start\n",
    "    span_id_1 = tracer.on_agent_start(\"Researcher\", RESEARCHER_PROMPT)\n",
    "    tracer.on_thought(\"Researcher\", f\"I need to find facts about {topic}.\")\n",
    "    tracer.on_action(\"Researcher\", \"Searching internal knowledge base...\")\n",
    "    research_output = f\"Key facts about {topic}: 1. It is a complex system. 2. It involves multiple agents.\"\n",
    "    tracer.on_output(\"Researcher\", research_output)\n",
    "    tracer._create_transfer_span(\"Researcher\", \"Writer\")\n",
    "    tracer.on_agent_end(\"Researcher\", research_output)\n",
    "\n",
    "    # 2. Writer Step\n",
    "    print(\"\\n--- Writer Working ---\")\n",
    "    span_id_2 = tracer.on_agent_start(\"Writer\", WRITER_PROMPT)\n",
    "    tracer.on_thought(\"Writer\", \"I have received facts. Now I must summarize them.\")\n",
    "    final_summary = f\"The {topic} is characterized by its complexity and multi-agent nature.\"\n",
    "    tracer.on_output(\"Writer\", final_summary)\n",
    "    tracer.on_agent_end(\"Writer\", final_summary)\n",
    "    \n",
    "    print(\"\\n‚úÖ Scenario Complete\")\n",
    "    return tracer.get_spans()\n",
    "\n",
    "# Run the scenario\n",
    "spans = await run_scenario(\"Autonomous Swarms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph Construction (DAG)\n",
    "Convert linear traces into a Causal Reasoning Graph (CRG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_interaction_graph(spans: List[Span]) -> nx.DiGraph:\n",
    "    crg_module = CRGModule()\n",
    "    graph = crg_module.build(spans)\n",
    "    \n",
    "    # Add semantic edges for richer analysis\n",
    "    num_semantic = crg_module.add_semantic_edges(similarity_threshold=0.5)\n",
    "    print(f\"Graph constructed: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges\")\n",
    "    print(f\"Added {num_semantic} semantic edges\")\n",
    "    \n",
    "    return graph\n",
    "\n",
    "graph = build_interaction_graph(spans)\n",
    "\n",
    "# Visualize\n",
    "visualizer = GraphVisualizer(graph)\n",
    "visualizer.plot(output_path=\"interaction_graph.png\")\n",
    "# plt.imshow(plt.imread(\"interaction_graph.png\"))\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GEMMAS Evaluation\n",
    "Calculate Information Diversity Score (IDS) and Unnecessary Path Ratio (UPR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = GEMMAS_Evaluator()\n",
    "metrics = evaluator.evaluate(graph, spans)\n",
    "\n",
    "print(\"=== GEMMAS Metrics ===\")\n",
    "print(f\"IDS (Information Diversity): {metrics['IDS']:.4f} ({metrics['IDS_interpretation']})\")\n",
    "print(f\"UPR (Unnecessary Paths):     {metrics['UPR']:.4f} ({metrics['UPR_interpretation']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MAST Analysis (Self-Correction)\n",
    "Automatically check for a fine-tuned judge. If missing, self-train (fine-tune) one using the MAST dataset, then classify failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_mast_analysis(spans: List[Span]):\n",
    "    CONFIG_PATH = \"tuned_mast_judge.json\"\n",
    "    \n",
    "    # 1. Check for existing model\n",
    "    if os.path.exists(CONFIG_PATH):\n",
    "        print(\"‚úÖ Found existing fine-tuned MAST judge.\")\n",
    "        train_result = MASTFineTuner.load_model_config(CONFIG_PATH)\n",
    "        model_name = train_result.model_name\n",
    "        mode = ClassifierMode.FINE_TUNED\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No fine-tuned judge found. Initiating self-training...\")\n",
    "        # Initialize Tuner\n",
    "        tuner = MASTFineTuner()\n",
    "        dataset_path = \"mast_dataset/MAD_human_labelled_dataset.json\"\n",
    "        \n",
    "        if not os.path.exists(dataset_path):\n",
    "             # Fallback if dataset not found on disk (mock logic for demo reliability)\n",
    "             print(\"   Dataset not found. Using Few-Shot ICL mode instead of training.\")\n",
    "             model_name = \"gemini-2.5-flash\"\n",
    "             mode = ClassifierMode.FEW_SHOT_ICL\n",
    "        else:\n",
    "            # Train\n",
    "            # Note: Running training in a notebook cell blocks until completion\n",
    "            print(\"   Preparing dataset and starting fine-tuning job...\")\n",
    "            tuner.prepare_training_data(dataset_path, max_examples=50) # Small batch for demo\n",
    "            result = tuner.start_training(wait=True)\n",
    "            tuner.save_model_config(CONFIG_PATH)\n",
    "            model_name = result.model_name\n",
    "            mode = ClassifierMode.FINE_TUNED\n",
    "    \n",
    "    # 2. Classify Failures\n",
    "    print(f\"\\nüîç Analyzing traces using {model_name} ({mode.value})...\")\n",
    "    classifier = MASTClassifier(model=model_name, mode=mode)\n",
    "    result = classifier.classify(spans)\n",
    "    \n",
    "    print(classifier.summary(result))\n",
    "    return result\n",
    "\n",
    "mast_result = await run_mast_analysis(spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Report & Advisor (Gemini 2.5)\n",
    "Synthesize all findings into a report and ask the \"Senior MAS Architect\" (Gemini 2.5) for specific improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def generate_advisory_report(metrics, mast_result, graph_path=\"interaction_graph.png\"):\n",
    "    # 1. Compile Data\n",
    "    report_data = {\n",
    "        \"metrics\": metrics,\n",
    "        \"failures\": [f.to_dict() if hasattr(f, 'to_dict') else str(f) for f in mast_result.failure_modes],\n",
    "        \"trace_summary\": mast_result.trace_summary\n",
    "    }\n",
    "    \n",
    "    report_json = json.dumps(report_data, indent=2)\n",
    "    \n",
    "    # 2. Prompt Gemini 2.5 as \"Senior MAS Architect\"\n",
    "    architect_prompt = f\"\"\"\n",
    "    You are an expert Multi-Agent System (MAS) Architect.\n",
    "    \n",
    "    Analyze the following evaluation report for a MAS execution:\n",
    "    {report_json}\n",
    "    \n",
    "    The Interaction Graph is attached (if available).\n",
    "    \n",
    "    Task:\n",
    "    1. Analyze the GEMMAS metrics (IDS, UPR).\n",
    "    2. Review the MAST failure modes detected.\n",
    "    3. Recommend 3 specific, actionable changes to the agent prompts, topology, or logic to improve efficiency and reduce failures.\n",
    "    \n",
    "    Format your response as a professional Engineering Advisory Memo.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ü§î Asking Senior MAS Architect for advice...\")\n",
    "    \n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "        # If we had the image loaded as a PIL object or bytes, we would pass it here.\n",
    "        # For now, we pass the text context.\n",
    "        response = model.generate_content(architect_prompt)\n",
    "        \n",
    "        display(Markdown(\"# üèóÔ∏è Senior MAS Architect Advisory Report\"))\n",
    "        display(Markdown(response.text))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating advisory: {e}\")\n",
    "\n",
    "generate_advisory_report(metrics, mast_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
